{"cells": [{"metadata": {}, "cell_type": "code", "source": "import random\nrddX = sc.parallelize(random.sample(list(range(100)),100))\nrddY = sc.parallelize(random.sample(list(range(100)),100))", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191128023527-0000\nKERNEL_ID = 7bcec239-210a-4b40-810e-9f4141755608\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we calculate the mean, note that we explicitly cast the denominator to float in order to obtain a float instead of int"}, {"metadata": {}, "cell_type": "code", "source": "meanX = rddX.sum()/float(rddX.count())\nmeanY = rddY.sum()/float(rddY.count())\nprint(meanX)\nprint(meanY)", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "49.5\n49.5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we calculate the covariance."}, {"metadata": {}, "cell_type": "code", "source": "rddXY = rddX.zip(rddY)\ncovXY = rddXY.map(lambda x_y : (x_y[0]-meanX)*(x_y[1]-meanY)).sum()/rddXY.count()\ncovXY", "execution_count": 6, "outputs": [{"output_type": "execute_result", "execution_count": 6, "data": {"text/plain": "-16.83"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Correlation and Standard Deviations:"}, {"metadata": {}, "cell_type": "code", "source": "from math import sqrt\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x: pow(x-meanX, 2)).sum()/n)\nsdY = sqrt(rddY.map(lambda x: pow(x-meanY, 2)).sum()/n)\nprint(sdX)\nprint(sdY)", "execution_count": 7, "outputs": [{"output_type": "stream", "text": "28.86607004772212\n28.86607004772212\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "corrXY = covXY/(sdX * sdY)\ncorrXY", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "-0.020198019801980195"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics\nimport random\ncolumn1 = sc.parallelize(range(100))\ncolumn2 = sc.parallelize(range(100,200))\ncolumn3 = sc.parallelize(list(reversed(range(100))))\ncolumn4 = sc.parallelize(random.sample(range(100),100))\ndata = column1.zip(column2).zip(column3).zip(column4).map(lambda a_b_c_d : (a_b_c_d[0][0][0],a_b_c_d[0][0][1],a_b_c_d[0][1],a_b_c_d[1]) ).map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]])\nprint(Statistics.corr(data))", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "[[ 1.          1.         -1.          0.05648965]\n [ 1.          1.         -1.          0.05648965]\n [-1.         -1.          1.         -0.05648965]\n [ 0.05648965  0.05648965 -0.05648965  1.        ]]\n", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}